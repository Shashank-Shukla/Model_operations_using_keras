{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load and Export Models in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook works with TensorFlow version: 2.0.1\n",
      "['.ipynb_checkpoints', 'models', 'model_name', 'Student Notebook.ipynb', 'tmp', 'weights']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print('This notebook works with TensorFlow version:', tf.__version__)\n",
    "\n",
    "folders = ['tmp', 'models', 'model_name', 'weights']\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # 1st layer is mnist dataset\n",
    "    # 3rd layer is output layer with 10 classes\n",
    "    model = tf.keras.models.Sequential([tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "                                      tf.keras.layers.Dense(128, activation='relu'),\n",
    "                                      tf.keras.layers.Dense(10,activation='softmax')])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
    "\n",
    "# one-hot encoding the lables\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Model Checkpoint During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.6938 - acc: 0.7637 - val_loss: 0.5052 - val_acc: 0.8182\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.4352 - acc: 0.8468 - val_loss: 0.4473 - val_acc: 0.8385\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.3876 - acc: 0.8628 - val_loss: 0.4124 - val_acc: 0.8505\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.3542 - acc: 0.8734 - val_loss: 0.3918 - val_acc: 0.8612\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.3317 - acc: 0.8804 - val_loss: 0.3613 - val_acc: 0.8713\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.3141 - acc: 0.8867 - val_loss: 0.3627 - val_acc: 0.8688\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.3056 - acc: 0.8899 - val_loss: 0.3542 - val_acc: 0.8749\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.2907 - acc: 0.8941 - val_loss: 0.3573 - val_acc: 0.8714\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2782 - acc: 0.8982 - val_loss: 0.3389 - val_acc: 0.8767\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2682 - acc: 0.9019 - val_loss: 0.3437 - val_acc: 0.8811\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2634 - acc: 0.9034 - val_loss: 0.3349 - val_acc: 0.8787\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2555 - acc: 0.9060 - val_loss: 0.3270 - val_acc: 0.8846\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.2445 - acc: 0.9100 - val_loss: 0.3471 - val_acc: 0.8758\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2403 - acc: 0.9114 - val_loss: 0.3391 - val_acc: 0.8769\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2322 - acc: 0.9136 - val_loss: 0.3281 - val_acc: 0.8845\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2262 - acc: 0.9174 - val_loss: 0.3412 - val_acc: 0.8777\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.2213 - acc: 0.9193 - val_loss: 0.3428 - val_acc: 0.8843\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2174 - acc: 0.9207 - val_loss: 0.3325 - val_acc: 0.8867\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2127 - acc: 0.9232 - val_loss: 0.3298 - val_acc: 0.8846\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2056 - acc: 0.9244 - val_loss: 0.3184 - val_acc: 0.8908\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2016 - acc: 0.9260 - val_loss: 0.3330 - val_acc: 0.8847\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1980 - acc: 0.9263 - val_loss: 0.3268 - val_acc: 0.8875\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1908 - acc: 0.9294 - val_loss: 0.3363 - val_acc: 0.8877\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1876 - acc: 0.9305 - val_loss: 0.3477 - val_acc: 0.8828\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1858 - acc: 0.9318 - val_loss: 0.3276 - val_acc: 0.8898\n"
     ]
    }
   ],
   "source": [
    "# We can save model weights using model checkpoint callbacks\n",
    "checkpoint_dir = 'weights/'\n",
    "epochs=25\n",
    "batch_size=512\n",
    "# callbacks allow us to do model checkpoint during training\n",
    "# Weights are saved as .h5 format\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_dir,'epoch_{epoch:02d}_acc_{val_acc:.2f}'),monitor='val_acc', save_weights_only=True, save_best_only=True)]\n",
    "# save_weights_only saves model weights only and not model architecture\n",
    "_ = model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint',\n",
       " 'epoch_01_acc_0.82.data-00000-of-00001',\n",
       " 'epoch_01_acc_0.82.index',\n",
       " 'epoch_02_acc_0.84.data-00000-of-00001',\n",
       " 'epoch_02_acc_0.84.index',\n",
       " 'epoch_03_acc_0.85.data-00000-of-00001',\n",
       " 'epoch_03_acc_0.85.index',\n",
       " 'epoch_04_acc_0.86.data-00000-of-00001',\n",
       " 'epoch_04_acc_0.86.index',\n",
       " 'epoch_05_acc_0.87.data-00000-of-00001',\n",
       " 'epoch_05_acc_0.87.index',\n",
       " 'epoch_07_acc_0.87.data-00000-of-00001',\n",
       " 'epoch_07_acc_0.87.index',\n",
       " 'epoch_09_acc_0.88.data-00000-of-00001',\n",
       " 'epoch_09_acc_0.88.index',\n",
       " 'epoch_10_acc_0.88.data-00000-of-00001',\n",
       " 'epoch_10_acc_0.88.index',\n",
       " 'epoch_12_acc_0.88.data-00000-of-00001',\n",
       " 'epoch_12_acc_0.88.index',\n",
       " 'epoch_18_acc_0.89.data-00000-of-00001',\n",
       " 'epoch_18_acc_0.89.index',\n",
       " 'epoch_20_acc_0.89.data-00000-of-00001',\n",
       " 'epoch_20_acc_0.89.index']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3604580329895017, 0.1266]\n"
     ]
    }
   ],
   "source": [
    "# Create a blank instance [random weights]->untrained model\n",
    "model = create_model()\n",
    "print(model.evaluate(x_test, y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3184475895166397, 0.8908]\n"
     ]
    }
   ],
   "source": [
    "# Don't load the data file, load the index file and remove .index from the path\n",
    "model.load_weights('weights/epoch_20_acc_0.89')\n",
    "print(model.evaluate(x_test, y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Saving Complete Model During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.6939 - acc: 0.7688 - val_loss: 0.4891 - val_acc: 0.8251\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.4295 - acc: 0.8501 - val_loss: 0.4301 - val_acc: 0.8457\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3885 - acc: 0.8621 - val_loss: 0.4063 - val_acc: 0.8541\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.3596 - acc: 0.8713 - val_loss: 0.3952 - val_acc: 0.8566\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.3412 - acc: 0.8773 - val_loss: 0.3685 - val_acc: 0.8673\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3238 - acc: 0.8833 - val_loss: 0.3566 - val_acc: 0.8725\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.3072 - acc: 0.8895 - val_loss: 0.3659 - val_acc: 0.8675\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.2935 - acc: 0.8931 - val_loss: 0.3502 - val_acc: 0.8728\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2824 - acc: 0.8970 - val_loss: 0.3540 - val_acc: 0.8760\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2777 - acc: 0.8989 - val_loss: 0.3354 - val_acc: 0.8807\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2706 - acc: 0.9007 - val_loss: 0.3484 - val_acc: 0.8741\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2590 - acc: 0.9052 - val_loss: 0.3362 - val_acc: 0.8787\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2521 - acc: 0.9078 - val_loss: 0.3371 - val_acc: 0.8800\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.2476 - acc: 0.9099 - val_loss: 0.3282 - val_acc: 0.8803\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2436 - acc: 0.9105 - val_loss: 0.3276 - val_acc: 0.8854\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2333 - acc: 0.9140 - val_loss: 0.3305 - val_acc: 0.8838\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2288 - acc: 0.9155 - val_loss: 0.3325 - val_acc: 0.8851\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2237 - acc: 0.9171 - val_loss: 0.3302 - val_acc: 0.8856\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2189 - acc: 0.9187 - val_loss: 0.3250 - val_acc: 0.8834\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2124 - acc: 0.9216 - val_loss: 0.3317 - val_acc: 0.8839\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2048 - acc: 0.9251 - val_loss: 0.3285 - val_acc: 0.8904\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.2006 - acc: 0.9260 - val_loss: 0.3217 - val_acc: 0.8908\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.1955 - acc: 0.9279 - val_loss: 0.3230 - val_acc: 0.8881\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.1932 - acc: 0.9287 - val_loss: 0.3266 - val_acc: 0.8906\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.1895 - acc: 0.9301 - val_loss: 0.3469 - val_acc: 0.8832\n"
     ]
    }
   ],
   "source": [
    "checkpoint_model_dir = 'models/'\n",
    "epochs=25\n",
    "batch_size=512\n",
    "\n",
    "# Let's initialize a fresh instance of the model [untrained model]\n",
    "model=create_model()\n",
    "# Weights can be saved in any format, preferrably .h5\n",
    "callbacks=[tf.keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_model_dir,\n",
    "                                                           'epoch_{epoch:02d}_acc_{val_acc:.2f}.h5'),\n",
    "                                              monitor='val_acc', \n",
    "                                              save_weights_only=False, \n",
    "                                              save_best_only=True)]\n",
    "# save_weights_only=False saves model weights with model architecture\n",
    "_ = model.fit(x_train,y_train, validation_data=(x_test,y_test), epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epoch_01_acc_0.83.h5',\n",
       " 'epoch_02_acc_0.85.h5',\n",
       " 'epoch_03_acc_0.85.h5',\n",
       " 'epoch_04_acc_0.86.h5',\n",
       " 'epoch_05_acc_0.87.h5',\n",
       " 'epoch_06_acc_0.87.h5',\n",
       " 'epoch_08_acc_0.87.h5',\n",
       " 'epoch_09_acc_0.88.h5',\n",
       " 'epoch_10_acc_0.88.h5',\n",
       " 'epoch_15_acc_0.89.h5',\n",
       " 'epoch_18_acc_0.89.h5',\n",
       " 'epoch_21_acc_0.89.h5',\n",
       " 'epoch_22_acc_0.89.h5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(checkpoint_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.475870908355713, 0.0531]\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh instance\n",
    "model=create_model()\n",
    "print(model.evaluate(x_test, y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32165690634250643, 0.8908]\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.load_model('models/epoch_22_acc_0.89.h5')\n",
    "print(model.evaluate(x_test, y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Manually Saving Weights and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint',\n",
       " 'manually_saved.w.data-00000-of-00001',\n",
       " 'manually_saved.w.index']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving just the weights\n",
    "model.save_weights('tmp/manually_saved.w')\n",
    "os.listdir('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint',\n",
       " 'manually_saved.h5',\n",
       " 'manually_saved.w.data-00000-of-00001',\n",
       " 'manually_saved.w.index']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving an entire model\n",
    "model.save('tmp/manually_saved.h5')\n",
    "os.listdir('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Exporting and Restoring SavedModel Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 10:50:43.447345   640 deprecation.py:506] From c:\\users\\administrator\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['assets', 'saved_model.pb', 'variables']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path name = model_name here\n",
    "model.save('model_name')\n",
    "os.listdir('model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32114194617271424, 0.8908]\n"
     ]
    }
   ],
   "source": [
    "# Bringing in the already saved model into keras-model\n",
    "model = tf.keras.models.load_model('model_name')\n",
    "print(model.evaluate(x_test,y_test, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
